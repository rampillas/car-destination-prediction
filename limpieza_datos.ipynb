{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 13:15:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/20 13:15:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/opt/anaconda3/envs/redes_neuronales_tus/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.SQLContext(sc)\n",
    "\n",
    "working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet file into dataframe\n",
    "df = spark.read.parquet(\"file://\" + working_dir + \"/testDataset/part-00000-711fabb0-5efc-4d83-afad-0e03a3156794.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andres/opt/anaconda3/envs/redes_neuronales_tus/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/andres/opt/anaconda3/envs/redes_neuronales_tus/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine('postgresql://postgres@193.144.208.246:5432/tus_replica')\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    query = text('select * from viajes_noviembre')\n",
    "    df = pd.read_sql_query(query,conn)\n",
    "    df = spark.createDataFrame(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show total count of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count lines\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the aspect of fiew records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------------------+---------+------------+---------+------------+--------+----------+\n",
      "|    id|nodo_org|nodo_dest|   instante_tickado|linea_org|sublinea_org|viaje_org|tipo_usuario| tarjeta| fecha_dia|\n",
      "+------+--------+---------+-------------------+---------+------------+---------+------------+--------+----------+\n",
      "|125679|      39|        1|2022-11-01 05:50:22|       41|           1|        3|           1|ES202954|2022-11-01|\n",
      "|125680|      41|       45|2022-11-01 05:54:43|       41|           1|        3|          16|DP001458|2022-11-01|\n",
      "|125681|     509|       40|2022-11-01 06:03:10|        3|           1|        1|           3|PS403368|2022-11-01|\n",
      "|125682|     444|       60|2022-11-01 06:21:54|        3|           1|        1|           1|ES182742|2022-11-01|\n",
      "|125683|     444|      355|2022-11-01 06:22:38|        3|           1|        2|           1|ES202471|2022-11-01|\n",
      "+------+--------+---------+-------------------+---------+------------+---------+------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 5 lines\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show all types of eventType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"rawdata\")\n",
    "spark.sql(\"SELECT DISTINCT eventType FROM rawdata\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with null or NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with any null or NaN value\n",
    "df.dropna\n",
    "# Count lines\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update created temp view for raw data\n",
    "df.createOrReplaceTempView(\"rawdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary event types\n",
    "Since we are going to predict estimation based on the start position, we won't need the rest of events (location, etc). Also, we remove rows with gps non valid values (0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create a view with only the values we need for processing\n",
    "df = spark.sql(\"SELECT * FROM rawdata ORDER BY instante_tickado\")\n",
    "df.createOrReplaceTempView(\"cleands\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data transformation for a single device\n",
    "The purpose is to reduce the problem so that we are sure that the transformations that we apply to the data produce the desired results. \n",
    "The aim of these transformations is to clean the dataset and take only the correctly paired start-trip / end-trip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------------------+---------+------------+---------+------------+--------+----------+\n",
      "|id    |nodo_org|nodo_dest|instante_tickado   |linea_org|sublinea_org|viaje_org|tipo_usuario|tarjeta |fecha_dia |\n",
      "+------+--------+---------+-------------------+---------+------------+---------+------------+--------+----------+\n",
      "|125682|444     |60       |2022-11-01 06:21:54|3        |1           |1        |1           |ES182742|2022-11-01|\n",
      "|127524|60      |511      |2022-11-01 17:38:34|3        |3           |3        |1           |ES182742|2022-11-01|\n",
      "|127763|39      |494      |2022-11-06 06:31:27|3        |1           |2        |1           |ES182742|2022-11-06|\n",
      "|99022 |304     |305      |2022-11-09 14:17:26|12       |1           |5        |1           |ES182742|2022-11-09|\n",
      "|99038 |305     |309      |2022-11-09 14:19:16|12       |1           |6        |1           |ES182742|2022-11-09|\n",
      "|137753|305     |301      |2022-11-17 13:53:26|12       |1           |6        |1           |ES182742|2022-11-17|\n",
      "|146533|301     |304      |2022-11-21 07:49:11|241      |2           |1        |1           |ES182742|2022-11-21|\n",
      "|107980|301     |305      |2022-11-23 07:46:46|241      |2           |1        |1           |ES182742|2022-11-23|\n",
      "|65207 |301     |306      |2022-11-24 07:52:29|241      |2           |1        |1           |ES182742|2022-11-24|\n",
      "|69800 |306     |311      |2022-11-24 13:31:09|242      |2           |6        |1           |ES182742|2022-11-24|\n",
      "|25140 |301     |304      |2022-11-28 10:42:16|241      |2           |3        |1           |ES182742|2022-11-28|\n",
      "|27436 |304     |305      |2022-11-28 14:16:23|12       |1           |5        |1           |ES182742|2022-11-28|\n",
      "|27450 |305     |311      |2022-11-28 14:17:46|12       |1           |6        |1           |ES182742|2022-11-28|\n",
      "+------+--------+---------+-------------------+---------+------------+---------+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show an example of data for a certain device\n",
    "singleDevice = spark.sql(\"SELECT * FROM cleands WHERE tarjeta='ES182742'\")\n",
    "singleDevice.createOrReplaceTempView(\"singledevice\")\n",
    "singleDevice.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract previous and next event for every row\n",
    "To do this, we use LAG and LEAD window functions to shift the data. The result will add two new columns *previousEvent* and *nextEvent* to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift values of eventType to add previous and next event\n",
    "singleDevice = spark.sql(\"SELECT *, LAG(eventType, 1) OVER (ORDER BY instante_tickado) AS previousEvent, \\\n",
    "LEAD(eventType, 1) OVER (ORDER BY instante_tickado) AS nextEvent FROM singledevice\")\n",
    "singleDevice.createOrReplaceTempView(\"singledevice\")\n",
    "singleDevice.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove trip-start and trip-end non valid rows\n",
    "The only valid data is that which defines a valid trip, that is, a trip-start followed by a trip-end event. Those trip-start with no trip-end after it, or those trip-end without a preceding trip-start are removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non valid trip-start and trip-end (those without its corresponding pair)\n",
    "singleDevice = spark.sql(\"SELECT deviceId, eventTime, lat, lon, eventType FROM singledevice \\\n",
    "WHERE NOT ((eventType='trip-end' AND previousEvent='trip-end') \\\n",
    "OR (eventType='trip-start' AND nextEvent='trip-start'))\")\n",
    "singleDevice.createOrReplaceTempView(\"singledevice\")\n",
    "singleDevice.show(20, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Put all the data of every complete trip in the same row\n",
    "We move the position and time data of every complete trip in a single row. This way we will be able to process each row as a trip, and we'll no longer need deviceId or eventType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move start and end values of a trip to a single row. We won't need deviceId any more\n",
    "singleDevice = spark.sql(\"SELECT eventTime AS eventTimeStart, \\\n",
    "LEAD(eventTime, 1) OVER (ORDER BY eventTime) AS eventTimeEnd, lat AS latStart, lon AS lonStart, \\\n",
    "LEAD(lat, 1) OVER (ORDER BY eventTime) AS latEnd, \\\n",
    "LEAD(lon, 1) OVER (ORDER BY eventTime) AS lonEnd, \\\n",
    "eventType FROM singledevice\")\n",
    "singleDevice.createOrReplaceTempView(\"singledevice\")\n",
    "singleDevice = spark.sql(\"SELECT eventTimeStart, eventTimeEnd, latStart, lonStart, latEnd, lonEnd \\\n",
    "FROM singledevice WHERE eventType='trip-start'\")\n",
    "singleDevice.createOrReplaceTempView(\"singledevice\")\n",
    "singleDevice.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the transformations to the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract previous and next event for every row\n",
    "To do this, we use LAG and LEAD window functions to shift the data. The result will add two new columns *previousEvent* and *nextEvent* to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift values of eventType to add previous and next event\n",
    "allDevices = spark.sql(\"SELECT *, LAG(eventType, 1) OVER (PARTITION BY deviceId ORDER BY eventTime) AS previousEvent, \\\n",
    "LEAD(eventType, 1) OVER (PARTITION BY deviceId ORDER BY eventTime) AS nextEvent FROM cleands\")\n",
    "allDevices.createOrReplaceTempView(\"alldevices\")\n",
    "allDevices.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove trip-start and trip-end non valid rows\n",
    "The only valid data is that which defines a valid trip, that is, a trip-start followed by a trip-end event. Those trip-start with no trip-end after it, or those trip-end without a preceeding trip-start are removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non valid trip-start and trip-end (those without its corresponding pair)\n",
    "allDevices = spark.sql(\"SELECT deviceId, eventTime, lat, lon, eventType FROM alldevices \\\n",
    "WHERE NOT ((eventType='trip-end' AND previousEvent='trip-end') \\\n",
    "OR (eventType='trip-start' AND nextEvent='trip-start'))\")\n",
    "allDevices.createOrReplaceTempView(\"alldevices\")\n",
    "allDevices.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDevices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Put all the data of every complete trip in the same row\n",
    "We move the position and time data of every complete trip in a single row. This way we will be able to process each row as a trip, and we'll no longer need deviceId or eventType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move start and end values of a trip to a single row. We won't need deviceId any more\n",
    "allDevices = spark.sql(\"SELECT eventTime AS eventTimeStart, \\\n",
    "LEAD(eventTime, 1) OVER (PARTITION BY deviceId ORDER BY eventTime) AS eventTimeEnd, lat AS latStart, lon AS lonStart, \\\n",
    "LEAD(lat, 1) OVER (PARTITION BY deviceId ORDER BY eventTime) AS latEnd, \\\n",
    "LEAD(lon, 1) OVER (PARTITION BY deviceId ORDER BY eventTime) AS lonEnd, \\\n",
    "eventType FROM alldevices\")\n",
    "allDevices.createOrReplaceTempView(\"alldevices\")\n",
    "allDevices = spark.sql(\"SELECT eventTimeStart, eventTimeEnd, latStart, lonStart, latEnd, lonEnd \\\n",
    "FROM alldevices WHERE eventType='trip-start'\")\n",
    "allDevices.createOrReplaceTempView(\"alldevices\")\n",
    "allDevices.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDevices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates\n",
    "After examining the resulting data, we observe that there many registers with exactly the same data for the time start and de starting and end position, so we remove the redundant data to avoid bias on the predicting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDevices = allDevices.dropDuplicates(['eventTimeStart', 'latStart', 'lonStart', 'latEnd', 'lonEnd'])\n",
    "allDevices.createOrReplaceTempView(\"alldevices\")\n",
    "allDevices.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDevices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfProcessed = allDevices.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check after transformations\n",
    "We make an overview of the description of the data before and after the processing to check that it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we describe the data after processing. We observe allmost all the position data for start and end is identical, so just looking at the data we could tell that our car is always doing the same trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProcessed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also describe the data before doing any processing. Starting and end positions are mixed, so we can make much conclusions, but we can observe that the range of the data make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRaw = df.toPandas()\n",
    "dfRaw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProcessed = dfProcessed.sort_values(\"eventTimeStart\")\n",
    "dfProcessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map visualization\n",
    "When handling geospatial data, is a good practise to visualize it in a map to find insights or avoid erroneous assumptions on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A google maps Javascript API KEY is needed to see the maps in the notebook\n",
    "import gmaps\n",
    "gmaps.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startLocs = dfProcessed[['latStart','lonStart']]\n",
    "endLocs = dfProcessed[['latEnd', 'lonEnd']]\n",
    "center_coords = (32.920782, -96.950904)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting positions visualization\n",
    "A heatmap is displayed to observe the density of the trip-start positions in a map. We can see that there are three main areas around Dallas area were the cars start their trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = gmaps.figure(center=center_coords, zoom_level=10)\n",
    "heatmap = gmaps.heatmap_layer(startLocs)\n",
    "heatmap.max_intensity = 2\n",
    "heatmap.point_radius = 20\n",
    "fig.add_layer(heatmap)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ending positions visualization\n",
    "As with starting positions, a heatmap is displayed to observe the density of the trip-end positions in a map. Again, there three main areas were the cars end their trips, showing that they likely do a go and return trip on the same areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = gmaps.figure(center=center_coords, zoom_level=10)\n",
    "heatmap = gmaps.heatmap_layer(endLocs)\n",
    "heatmap.max_intensity = 2\n",
    "heatmap.point_radius = 20\n",
    "fig.add_layer(heatmap)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the positions markers combined are shown. trip-start are shown in green and trip-end in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_coords = (32.874843, -96.947014)\n",
    "figMarkers = gmaps.figure(center=center_coords, zoom_level=10)\n",
    "trip_start_layer = gmaps.symbol_layer(\n",
    "startLocs, fill_color=\"rgba(0, 150, 0, 0.4)\",\n",
    "stroke_color=\"rgba(0, 150, 0, 0.4)\", scale=4\n",
    ")\n",
    "trip_end_layer = gmaps.symbol_layer(\n",
    "endLocs, fill_color=\"rgba(200, 0, 0, 0.4)\",\n",
    "stroke_color=\"rgba(200, 0, 0, 0.4)\", scale=4\n",
    ")\n",
    "#trip_start_layer = gmaps.symbol_layer(startLocs, fill_color=\"green\", stroke_color=\"green\", scale=3)\n",
    "#trip_end_layer = gmaps.symbol_layer(endLocs, , stroke_color=\"red\", scale=3, fill_color=None, fill_opacity=0.6)\n",
    "figMarkers.add_layer(trip_start_layer)\n",
    "figMarkers.add_layer(trip_end_layer)\n",
    "figMarkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleansing and processing we save the processed dataset to a csv file for its later use to build the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProcessed.to_csv('processed-dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
